# DDPG-

# Описание реализованного алгоритма

Реализованный алгоритм представляет собой DDPG (Deep Deterministic Policy Gradient), который предназначен для обучения агентов, работающих в непрерывных пространствах действий.

# Actor-Critic Architecture:

Актер (Actor) генерирует действия на основе текущего состояния окружения. Он принимает состояние как вход и выдает действие (действие нормализуется в диапазоне от -макс до +макс).

Критик (Critic) оценивлет значение действия, которое принимает агент, вычисляя Q-значение для пары состояние-действие. Это значение отражает ожидаемое вознаграждение агента, если он выполнит это действие из данного состояния.

Агент использует буфер памяти (deque) для хранения опыта, что позволяет ему повторно использовать ранее собранные данные для обучения. Это улучшает эффективность и стабильность обучения.

# Графики обучения

Графики, созданные после выполнения обучения, показывают следующее:

График наград по эпизодам: Отображает, как награды агента меняются по мере обучения. Как мы видим на графике ближе к 200 эпизоду получение наград стабилизируется и находится в районе -200 очков

График функции потерь:

Этот график показывает изменения в потере критика, что позволяет отслеживать эффективность обучения. Снижение потерь говорит о том, что Q-значения обучаются более точно.

# Выводы о работе агента

Эффективность обучения:

График наград демонстрирует устойчивый рост, это указывает на то, что агент способен эффективно изучать и улучшать свою стратегию в заданной среде.

Снижение функции потерь также указывает на стабильное обучение агента.

Таким образом, агент, обученный с использованием DDPG, демонстрирует способности к обучению в непрерывных задачах, но результаты могут варьироваться в зависимости от настройки и сложности среды.
